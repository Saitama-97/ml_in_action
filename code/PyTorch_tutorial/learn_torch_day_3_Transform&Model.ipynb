{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习PyTorch-Day3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# for mac\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# for cuda\n",
    "# device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms-变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据并不总是按照机器学习算法所需的形式出现，我们经常需要对数据执行变换，使其适合训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(root=\"data\", \n",
    "                           train=True,\n",
    "                           download=False,\n",
    "                           transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToTensor可以将一张图像或者Numpy数组转换成 __FloatTensor__ ，并将图像的像素强度值缩放在[0., 1.] 范围内"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda可以封装任何自定义的变换函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络由各种层/模块构成，这些层/模块包装了对数据的操作\n",
    "- __torch.nn__ 中包括了构建神经网络所需的各种模板，PyTorch模板中的每个模块都是 __nn.Module__ 的子集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例：构建分类网络，对FashionMNIST数据集进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 指认设备\n",
    "device = torch.device(\n",
    "    \"cuda\" \n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" \n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义神经网络（继承于 __nn.Module__）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # 展平处理\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 全连接+激活\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 实例化网络，指认到设备上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初始化输入\n",
    "- 因为此问题是互斥分类问题，即结果只可能属于一类，所以应使用 Softmax 激活函数\n",
    "- nn.Softmax(dim=1)，此处的 dim 表示在哪个维度上 和为1，此处应该是10类输出之和1，所以应为 dim=1\n",
    "- Tensor.argmax() == torch.argmax(Tensor, dim)，返回输入Tensor中最大值的下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(1, 28, 28, device=device)\n",
    "out = model(t)\n",
    "# 因为此问题是互斥分类问题，即结果只可能属于一类，所以应使用 Softmax 激活函数\n",
    "softmax_func = nn.Softmax(dim=1)  # 此处的 dim 表示在哪个维度上 和为1，此处应该是10类输出之和1，所以应为 dim=1\n",
    "pred_prob = softmax_func(out)\n",
    "# argmax\n",
    "pred_cls = torch.argmax(pred_prob, dim=1)\n",
    "print(pred_cls.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 具体分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 以一张图像为例\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "# shape是Tensor的属性，size()是Tensor的方法，二者均可以获得Tensor的维度\n",
    "print(input_image.shape)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __nn.Flatten__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以将 2D图像展平，变成一个拥有 784个像素（连续）的数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "\n",
    "flat_img = flatten(input_image)\n",
    "print(flat_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __nn.Linear__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性层是一个使用其存储的权重和偏差对输入应用线性变换的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden = layer1(flat_img)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __nn.ReLU__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非线性激活函数在输入和输出之间建立复杂的映射关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它们在线性变换后应用以引入非线性，帮助神经网络学习各种现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.1219, -0.3522, -0.7829,  0.2842, -0.1779, -0.4762, -0.6249,  0.0191,\n",
      "          0.0280,  0.4442, -0.1072, -0.0033, -0.1826, -0.6254,  0.1005, -0.0075,\n",
      "         -0.3307, -0.0596, -0.2938,  0.2695],\n",
      "        [-0.3704, -0.4180, -0.4482,  0.1459, -0.4321,  0.0838, -0.6310, -0.0928,\n",
      "          0.2291,  0.2921, -0.1476,  0.0014, -0.2576, -0.7668,  0.0605, -0.4167,\n",
      "         -0.2690,  0.1728, -0.0496,  0.4177],\n",
      "        [-0.0715, -0.3375, -0.6984, -0.1599, -0.1517, -0.2152, -0.8183, -0.0715,\n",
      "          0.1835,  0.2847, -0.2910,  0.0037, -0.3580, -0.7739,  0.2845, -0.3604,\n",
      "         -0.0760, -0.1074, -0.4091,  0.5348]], grad_fn=<AddmmBackward0>)\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.2842, 0.0000, 0.0000, 0.0000, 0.0191, 0.0280,\n",
      "         0.4442, 0.0000, 0.0000, 0.0000, 0.0000, 0.1005, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2695],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1459, 0.0000, 0.0838, 0.0000, 0.0000, 0.2291,\n",
      "         0.2921, 0.0000, 0.0014, 0.0000, 0.0000, 0.0605, 0.0000, 0.0000, 0.1728,\n",
      "         0.0000, 0.4177],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1835,\n",
      "         0.2847, 0.0000, 0.0037, 0.0000, 0.0000, 0.2845, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.5348]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before ReLU: {}\".format(hidden))\n",
    "hidden1 = nn.ReLU(inplace=True)(hidden)\n",
    "print(\"After ReLU: {}\".format(hidden1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __nn.Sequential__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential是存放模块的有序容器，数据按照模块被定义的顺序经过各个模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "result = seq_modules(input_image)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __nn.Softmax__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络最后一层的返回值，送入 __nn.Softmax__ 模块，值被缩放到 [0, 1]，表示模型对每个类别的预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)  # dim=1表示对输入Tensor的dim=1维度进行Softmax处理，sum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1133, 0.0926, 0.0981, 0.0967, 0.0957, 0.1005, 0.1000, 0.0851, 0.1183,\n",
      "         0.0997],\n",
      "        [0.1286, 0.1006, 0.0962, 0.0928, 0.0914, 0.1084, 0.0974, 0.0858, 0.1067,\n",
      "         0.0921],\n",
      "        [0.1126, 0.0960, 0.1099, 0.0994, 0.0970, 0.1117, 0.0957, 0.0864, 0.1075,\n",
      "         0.0836]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred_prob = softmax(result)\n",
    "print(pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 遍历模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Architecture: {}\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml/lib/python3.7/site-packages/torch/_tensor_str.py:103: UserWarning: The operator 'aten::bitwise_and.Tensor_out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484744261/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  nonzero_finite_vals = torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: linear_relu_stack.0.weight, parameter: Parameter containing:\n",
      "tensor([[ 1.6962e-02,  3.2612e-02, -6.2463e-03,  ...,  2.8459e-03,\n",
      "          7.1751e-03,  2.4258e-02],\n",
      "        [-8.0782e-03, -2.1115e-02, -1.3490e-02,  ..., -1.0968e-02,\n",
      "          3.4302e-02, -1.7684e-02],\n",
      "        [ 9.9019e-03, -5.1077e-05, -3.5250e-02,  ...,  6.1434e-03,\n",
      "         -2.5816e-04, -2.1763e-02],\n",
      "        ...,\n",
      "        [ 1.2548e-02, -2.8348e-02,  3.1291e-02,  ...,  8.0662e-03,\n",
      "          8.3008e-03, -1.7072e-02],\n",
      "        [-2.6389e-02,  2.0579e-02,  2.3972e-02,  ..., -2.6682e-02,\n",
      "         -1.8750e-02, -2.6942e-02],\n",
      "        [-1.5087e-02, -8.9340e-03, -2.8124e-02,  ..., -1.1601e-02,\n",
      "         -3.4300e-02, -3.4701e-02]], device='mps:0', requires_grad=True)\n",
      "name: linear_relu_stack.0.bias, parameter: Parameter containing:\n",
      "tensor([ 3.2329e-02,  1.9988e-02, -1.5272e-02,  6.3280e-03,  1.1474e-02,\n",
      "         1.2977e-02, -1.8993e-03,  1.6732e-02,  9.3199e-05,  3.0604e-02,\n",
      "         3.1565e-02, -2.0835e-02, -1.6370e-02,  3.4749e-02,  3.3866e-02,\n",
      "        -2.3252e-02,  6.5520e-03,  2.4207e-02,  6.6567e-03,  1.4734e-02,\n",
      "         1.2132e-03, -1.1067e-02,  1.2210e-04,  1.9914e-02, -2.3568e-02,\n",
      "        -4.5844e-03,  1.8806e-02,  2.9108e-02,  8.9800e-03, -6.3468e-03,\n",
      "        -3.2874e-02, -2.9009e-02, -3.1920e-02,  2.6331e-02,  2.3218e-02,\n",
      "         4.1230e-03,  2.6890e-02,  3.2851e-02,  2.6896e-02,  7.9094e-04,\n",
      "         1.9313e-02, -1.2261e-02, -1.0926e-02, -4.6482e-03, -1.7573e-02,\n",
      "        -1.4441e-02, -6.8282e-03, -3.0704e-03, -2.7939e-02, -4.9204e-03,\n",
      "         1.9909e-02, -1.0684e-02,  1.2550e-02, -3.2489e-02,  1.3230e-02,\n",
      "         2.5719e-02, -3.0128e-02,  2.5485e-02,  3.1316e-02,  2.6383e-02,\n",
      "         2.0981e-02, -1.3213e-02,  1.9013e-02,  2.4593e-02, -7.8775e-03,\n",
      "         6.2094e-03, -2.6840e-02, -2.3802e-02,  2.7583e-02, -2.0697e-02,\n",
      "         1.4637e-02, -3.3139e-02, -1.7939e-02, -1.8787e-02,  3.5556e-03,\n",
      "        -1.6750e-02,  2.3359e-02,  1.1466e-02, -1.9215e-02, -2.4635e-02,\n",
      "         2.5703e-02,  8.7982e-03,  1.3208e-02, -1.2457e-02,  1.3559e-02,\n",
      "         1.4647e-02, -6.2332e-03,  2.3873e-02,  2.1720e-02, -1.6820e-02,\n",
      "         1.0415e-02, -1.4174e-02,  1.4908e-02,  8.7685e-03, -2.6547e-02,\n",
      "         2.1754e-02,  1.7071e-02, -1.9653e-02, -7.1510e-03,  3.5567e-02,\n",
      "         1.5079e-02, -1.4903e-02, -2.4706e-02, -2.1818e-02,  9.2652e-03,\n",
      "         5.2944e-03,  1.9788e-02, -1.0525e-02, -5.8477e-03, -5.5921e-03,\n",
      "        -6.3489e-03,  5.4479e-03,  2.0138e-02,  1.3166e-02,  2.1593e-03,\n",
      "        -1.2293e-02,  1.0010e-02,  1.2954e-02, -2.2079e-02,  2.2545e-02,\n",
      "        -1.0806e-02,  2.5581e-02,  1.4757e-02, -6.0871e-03, -1.4193e-03,\n",
      "         1.2841e-02, -1.3375e-02, -3.3420e-03, -3.1065e-02,  2.4912e-03,\n",
      "        -1.2666e-03, -4.5216e-03,  9.2218e-03,  1.1184e-02, -3.7837e-03,\n",
      "         3.5578e-02,  2.0199e-03,  1.5459e-02, -2.5378e-02,  3.2030e-02,\n",
      "         3.2673e-02,  1.6731e-03, -2.4246e-02, -2.5583e-02, -9.4967e-03,\n",
      "        -7.3048e-04, -2.1688e-03,  1.4022e-02, -1.2239e-02, -1.6100e-02,\n",
      "        -9.8643e-03,  1.9678e-02,  3.0495e-02,  4.6414e-04, -2.2929e-02,\n",
      "         2.3934e-02, -1.7092e-03, -3.1105e-02, -1.3980e-02, -2.4498e-02,\n",
      "        -1.5843e-02,  1.8347e-02,  2.2814e-02,  3.2111e-02,  2.7894e-02,\n",
      "        -3.2016e-02,  1.5410e-02,  2.4664e-02,  1.9111e-02,  1.2779e-02,\n",
      "        -1.7059e-02, -1.3099e-02, -1.1790e-02, -1.6046e-02, -1.8128e-02,\n",
      "         1.2202e-02, -2.7092e-03,  4.6820e-03, -2.0825e-02, -3.3439e-02,\n",
      "        -2.9532e-02,  1.4898e-02,  3.0944e-02, -1.1304e-02,  1.8300e-02,\n",
      "        -1.3875e-02, -1.1004e-02, -2.8790e-02, -3.5012e-02, -3.2139e-02,\n",
      "         2.3698e-02, -1.2850e-02, -1.7254e-03, -4.8196e-03,  9.6769e-03,\n",
      "         3.2417e-02, -2.7139e-02,  3.3345e-02,  5.3915e-03,  2.0776e-02,\n",
      "         1.3642e-02, -1.0998e-02,  1.7685e-02, -1.0090e-02, -1.2725e-02,\n",
      "         2.2317e-03, -3.4639e-02,  2.3107e-02,  8.5628e-03, -1.0339e-02,\n",
      "        -5.7562e-04,  2.9868e-02,  5.3948e-03, -1.0662e-02, -2.3223e-02,\n",
      "         6.1634e-03,  1.7342e-02, -2.4880e-02,  3.4102e-02, -3.6230e-03,\n",
      "        -9.4487e-03, -1.7402e-02, -4.1224e-04, -1.2422e-02, -1.3317e-02,\n",
      "         2.7253e-02,  1.7608e-02, -2.4397e-02,  1.4818e-02,  8.1126e-03,\n",
      "        -2.1695e-02, -4.2602e-04,  6.0664e-03, -2.0226e-02, -2.9872e-03,\n",
      "        -3.1779e-02, -3.1727e-02,  3.1979e-02,  2.6452e-02,  1.7193e-02,\n",
      "         1.4581e-02, -3.3975e-02,  1.6288e-02,  3.3058e-02,  1.1412e-02,\n",
      "         2.3384e-04,  2.8289e-02, -1.7340e-02, -1.5064e-02,  1.6006e-02,\n",
      "        -2.6901e-03,  2.3499e-02,  2.0647e-02,  2.4011e-02,  2.0440e-02,\n",
      "         2.5171e-02, -1.0297e-02, -2.1667e-02, -4.8055e-03,  2.9057e-02,\n",
      "        -1.8271e-02,  1.6761e-02,  7.1989e-03,  1.3654e-02,  1.4502e-02,\n",
      "         2.9959e-02,  1.3324e-02,  2.6869e-02, -1.1469e-02, -1.3994e-02,\n",
      "        -5.0525e-03,  1.7512e-02,  1.3777e-02, -9.9009e-03,  5.5826e-03,\n",
      "         1.3345e-02, -4.4804e-03, -1.8627e-02,  3.5339e-02,  1.7680e-02,\n",
      "         2.3366e-02, -3.5098e-02,  1.0228e-02,  4.1721e-03,  9.1442e-03,\n",
      "        -5.6122e-03, -1.3189e-02, -2.4061e-02,  1.7241e-02,  2.6825e-02,\n",
      "        -2.1725e-02, -2.6411e-02, -1.3481e-03,  3.0761e-02, -1.8864e-02,\n",
      "        -3.4326e-02,  3.3812e-02, -2.4560e-02, -2.3450e-02,  1.3838e-02,\n",
      "         1.7922e-02, -6.1152e-03, -3.3052e-02, -2.2992e-02,  1.8819e-02,\n",
      "        -2.0588e-02, -2.3640e-02,  2.1886e-02,  3.4084e-02,  1.1818e-02,\n",
      "        -8.7104e-03,  2.3912e-03, -2.4407e-02,  1.3139e-02, -2.6292e-02,\n",
      "         6.6784e-03,  2.6014e-02,  2.9095e-02, -1.6464e-03, -1.0386e-02,\n",
      "        -3.7473e-03, -2.8243e-02,  6.4475e-03,  1.8497e-02,  1.0920e-02,\n",
      "         2.5459e-02, -6.8352e-03,  4.0447e-04, -1.5166e-02,  2.8418e-02,\n",
      "         1.8105e-02,  6.1183e-03, -2.4327e-02,  1.0624e-03,  8.5480e-03,\n",
      "         3.2870e-02,  1.0026e-02,  2.8297e-02,  8.7664e-03,  3.2830e-02,\n",
      "         1.4803e-02, -8.5485e-04, -1.2496e-02, -3.1272e-02,  2.5418e-02,\n",
      "        -3.5674e-02, -1.5887e-02,  3.3511e-02,  4.2994e-03,  2.1939e-02,\n",
      "        -3.0539e-02, -1.1980e-02,  1.7088e-02, -1.3969e-02,  2.8305e-02,\n",
      "        -3.1058e-02, -1.0643e-02,  3.4357e-02,  2.1232e-02,  2.1710e-02,\n",
      "         1.7895e-02, -2.6601e-02, -3.0007e-02,  6.0432e-03, -1.9704e-02,\n",
      "        -3.3473e-02, -1.6464e-02, -5.3509e-03,  1.4497e-02,  2.6407e-02,\n",
      "        -2.2270e-02, -5.8779e-03,  2.8876e-02, -1.8413e-02,  7.8081e-03,\n",
      "         2.9765e-02,  3.7268e-03,  9.0735e-04, -2.3824e-02,  3.0056e-02,\n",
      "         1.2019e-02,  3.5116e-04,  1.5163e-02, -2.1965e-02, -2.7854e-02,\n",
      "        -1.7032e-02, -2.1587e-02, -1.0686e-04, -1.7006e-02,  1.4595e-03,\n",
      "         2.2612e-02,  1.6479e-02, -2.6455e-02, -2.6212e-02,  1.9506e-02,\n",
      "        -2.4628e-02,  1.6705e-02,  2.4534e-02,  3.4689e-02, -8.6329e-03,\n",
      "        -2.8411e-02, -6.9632e-03, -1.3565e-02,  1.9287e-02,  1.2170e-02,\n",
      "        -3.0682e-02, -2.9666e-02, -1.2586e-02, -3.3630e-02, -3.2595e-02,\n",
      "        -2.8750e-03,  1.4517e-02, -9.5534e-03,  3.4588e-02, -2.9860e-02,\n",
      "         6.7825e-03,  1.1664e-02,  1.2088e-02, -1.1439e-02, -5.7747e-03,\n",
      "        -2.4366e-02, -4.7286e-03, -2.3971e-02,  2.9675e-02,  2.0997e-02,\n",
      "        -2.2652e-02,  3.3921e-03,  1.2577e-02, -1.6956e-02, -1.6911e-02,\n",
      "         1.2534e-02,  1.4169e-03,  3.0619e-02, -2.2583e-02,  2.7631e-02,\n",
      "        -2.4226e-02, -1.0662e-02, -2.2595e-02, -1.5006e-02, -3.3754e-04,\n",
      "         1.6513e-02, -7.4134e-03,  2.6538e-02,  2.2779e-02,  3.4830e-03,\n",
      "         2.1503e-02,  7.0228e-03,  2.3684e-02, -6.9018e-03,  3.5496e-02,\n",
      "        -3.5310e-02, -1.8418e-02, -1.8066e-02, -1.2618e-02,  3.1596e-02,\n",
      "        -1.0163e-02,  3.5147e-02, -6.8775e-03,  1.2216e-02,  1.2345e-02,\n",
      "         3.2909e-02,  1.0199e-02,  9.2160e-03, -3.4862e-02, -2.2184e-02,\n",
      "         7.9327e-03,  1.3840e-02, -1.1113e-02,  1.8112e-02,  2.6949e-03,\n",
      "         2.8620e-02, -1.9009e-02, -2.0732e-02,  2.4910e-02, -2.2620e-02,\n",
      "        -7.7185e-03,  1.7567e-02, -2.2417e-02, -1.7479e-03, -3.2795e-02,\n",
      "        -2.6219e-02,  5.1220e-03,  3.1487e-02,  1.3087e-02, -2.1720e-02,\n",
      "         1.3922e-02,  3.4188e-02,  1.3457e-02, -2.3546e-02, -3.5220e-02,\n",
      "         3.1402e-02, -2.8324e-02, -3.2256e-02,  3.3467e-03,  1.8033e-02,\n",
      "         2.5366e-02,  3.5492e-02, -2.2251e-02,  4.1503e-03,  1.3532e-02,\n",
      "         1.5817e-02, -2.5968e-02, -9.3306e-03, -2.0375e-02,  2.0619e-03,\n",
      "         3.4189e-02, -6.6247e-03, -1.6276e-02, -2.1700e-02,  1.3738e-02,\n",
      "        -1.0306e-02, -5.4290e-03], device='mps:0', requires_grad=True)\n",
      "name: linear_relu_stack.2.weight, parameter: Parameter containing:\n",
      "tensor([[ 0.0044, -0.0438, -0.0118,  ...,  0.0022, -0.0424, -0.0274],\n",
      "        [-0.0333, -0.0256, -0.0300,  ...,  0.0398,  0.0389, -0.0151],\n",
      "        [ 0.0284, -0.0372,  0.0252,  ...,  0.0075, -0.0106,  0.0293],\n",
      "        ...,\n",
      "        [ 0.0292, -0.0425,  0.0334,  ..., -0.0310, -0.0224, -0.0108],\n",
      "        [ 0.0334,  0.0382,  0.0214,  ...,  0.0149, -0.0401, -0.0033],\n",
      "        [ 0.0044, -0.0133, -0.0075,  ...,  0.0051, -0.0104, -0.0040]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "name: linear_relu_stack.2.bias, parameter: Parameter containing:\n",
      "tensor([ 2.1416e-02,  3.6340e-02,  1.1777e-02,  2.4616e-02,  2.6882e-02,\n",
      "         4.0728e-02,  3.6511e-02,  4.1372e-02,  2.5815e-02, -3.3158e-02,\n",
      "         4.9987e-04, -5.8569e-03,  1.6458e-02, -3.1482e-02,  5.6280e-03,\n",
      "        -4.1148e-03,  2.1868e-02,  2.4268e-02, -2.2858e-03,  3.6531e-02,\n",
      "         3.2378e-02,  2.0773e-02, -2.0264e-02, -3.7143e-02,  1.5297e-03,\n",
      "        -4.2632e-02, -1.0172e-02,  2.7609e-02,  1.8565e-02, -1.3771e-02,\n",
      "        -2.7220e-02, -3.2814e-02, -3.7855e-02,  2.8392e-02, -2.4156e-02,\n",
      "        -1.3557e-02, -4.3400e-02, -1.9386e-02, -1.6908e-02,  2.6113e-02,\n",
      "         1.3661e-02,  3.6114e-02,  6.8521e-03, -2.4847e-02,  8.8396e-03,\n",
      "        -3.7434e-02, -9.2847e-03, -4.3893e-02, -3.4843e-02,  1.6393e-02,\n",
      "         1.4813e-02, -2.0090e-02, -2.1289e-02, -3.9146e-02, -3.5468e-02,\n",
      "        -4.0009e-02, -3.5733e-02,  1.8655e-03, -2.7566e-02,  1.8834e-02,\n",
      "        -1.8094e-02, -3.9104e-02, -1.2278e-02,  3.3068e-02, -2.2882e-02,\n",
      "        -5.0363e-03,  9.9459e-03, -1.0306e-02,  1.2828e-02,  3.3391e-02,\n",
      "        -3.6719e-02,  3.3465e-02,  3.7788e-02,  1.4274e-02, -3.3613e-02,\n",
      "         1.1247e-02, -5.6682e-03,  7.8816e-03,  9.2835e-03,  3.5791e-02,\n",
      "         1.8099e-02, -3.3756e-02, -3.8243e-02, -1.4007e-02,  3.1665e-02,\n",
      "        -7.7937e-03, -3.1799e-03,  2.3893e-02, -7.0090e-03,  4.3070e-02,\n",
      "        -3.2983e-02,  3.8357e-02,  2.0050e-02, -2.2169e-02,  3.8035e-02,\n",
      "         9.6445e-03, -4.0631e-02,  4.4152e-02, -2.8324e-02, -2.8096e-02,\n",
      "        -5.5479e-03,  3.3067e-02, -3.3435e-02, -1.6070e-02, -2.1726e-02,\n",
      "         3.6587e-02,  2.3561e-02,  2.0907e-02,  4.0837e-03,  3.2335e-02,\n",
      "        -3.2814e-02,  3.8234e-02,  2.5172e-02, -3.6762e-02,  8.5203e-03,\n",
      "         1.6575e-02, -3.1491e-02, -4.0463e-02,  3.3309e-02, -1.7050e-02,\n",
      "        -1.9502e-03,  2.2043e-02, -3.4572e-02,  6.4686e-03,  3.4276e-02,\n",
      "         7.0907e-03,  2.8560e-02,  3.4358e-02, -3.2406e-02,  3.7635e-02,\n",
      "         8.5070e-04,  3.8396e-02,  1.1877e-02, -8.4076e-03, -3.8296e-02,\n",
      "        -1.5129e-03, -8.0744e-03, -2.8986e-02, -1.0655e-03, -2.2235e-02,\n",
      "        -1.3795e-02, -3.7512e-02,  2.1095e-02,  3.6685e-02,  1.3746e-03,\n",
      "        -2.9891e-02,  2.6882e-02, -1.8803e-02,  2.7072e-02,  1.8074e-02,\n",
      "        -1.1976e-02, -1.7273e-02,  3.7914e-02,  1.0137e-02, -1.9485e-02,\n",
      "         1.6608e-02, -2.2139e-03, -1.1736e-02,  2.7313e-02,  1.0311e-02,\n",
      "        -2.9660e-02, -7.8850e-03,  7.5945e-03, -4.3979e-02, -8.4109e-03,\n",
      "         5.0995e-04,  1.1178e-02, -3.5446e-02,  3.5660e-02, -9.9146e-03,\n",
      "        -1.0164e-02,  1.2165e-02, -3.8893e-02,  3.4726e-02, -8.1498e-03,\n",
      "         3.1684e-02, -2.4735e-02, -3.0294e-02, -1.8536e-02,  9.4258e-04,\n",
      "         4.1159e-02, -1.4784e-03, -2.7220e-03, -3.6558e-02,  3.2917e-02,\n",
      "         3.8786e-03, -3.3425e-02, -2.2172e-02, -1.0577e-03,  1.5541e-02,\n",
      "         4.1388e-02, -4.2691e-02,  2.3020e-02,  1.5416e-02,  2.1486e-02,\n",
      "        -3.6948e-02, -3.2746e-02, -2.2899e-03,  1.6408e-03, -7.7957e-03,\n",
      "        -5.7588e-03, -2.6438e-02,  2.2503e-02,  3.0978e-03,  2.7351e-02,\n",
      "         1.7701e-02,  3.7697e-02, -2.5683e-02, -2.0284e-02, -2.4829e-02,\n",
      "         1.5786e-02, -2.2046e-02,  1.0715e-02, -2.4981e-02,  1.2495e-02,\n",
      "         2.6923e-02,  4.4156e-03, -4.5391e-03, -1.3509e-02,  3.6060e-02,\n",
      "        -2.1142e-02,  1.1034e-02,  6.0885e-03,  4.3245e-02, -1.2125e-02,\n",
      "        -3.2602e-02,  8.0908e-03,  1.9921e-02, -3.4539e-02, -1.3214e-02,\n",
      "        -2.4314e-02, -1.6683e-02,  1.1094e-03,  2.8640e-02, -1.9920e-02,\n",
      "         4.0409e-03, -7.9057e-03,  3.1541e-02,  3.0035e-02, -3.0847e-02,\n",
      "         3.4189e-02,  3.1127e-03,  3.5406e-02,  1.2233e-02,  1.6495e-02,\n",
      "         3.0892e-02, -3.4877e-02, -1.4085e-02,  9.5733e-03,  3.7491e-02,\n",
      "        -3.5483e-02, -3.3700e-02, -3.5000e-02,  6.4735e-03, -1.3536e-02,\n",
      "        -2.4494e-02, -1.7128e-02,  3.6940e-03, -3.3940e-02,  1.4410e-02,\n",
      "         3.3731e-03, -2.1273e-02,  4.3894e-02, -2.6423e-02,  7.3399e-03,\n",
      "         3.6213e-02,  3.4293e-02,  2.0837e-02, -2.8162e-02, -1.4206e-02,\n",
      "         8.2101e-03, -4.2739e-02,  1.2810e-02, -3.0362e-02,  4.0130e-02,\n",
      "        -2.1901e-02, -4.9819e-03, -4.1137e-02, -2.3172e-02,  4.3733e-02,\n",
      "        -3.2588e-02, -6.6124e-03,  2.8229e-02, -4.3711e-02,  1.2079e-02,\n",
      "         2.0949e-02, -5.9989e-03, -2.2284e-02,  2.0065e-02,  1.6523e-02,\n",
      "        -3.7076e-02, -3.6392e-03,  1.8022e-02,  3.1400e-02, -2.6060e-02,\n",
      "        -4.1003e-02, -1.9195e-02, -8.7224e-03,  1.9282e-02,  2.9894e-02,\n",
      "         1.6955e-02, -3.6989e-02,  2.1136e-02, -1.0172e-02,  4.2694e-02,\n",
      "        -2.1827e-02,  2.8541e-02, -3.4477e-02, -2.0740e-02,  9.6914e-03,\n",
      "         4.0047e-02, -1.2990e-02, -3.3148e-02,  3.3021e-02,  3.5552e-02,\n",
      "         8.2698e-03, -4.3982e-02,  1.5507e-02,  2.6796e-02, -3.2501e-02,\n",
      "         2.4005e-02, -1.1670e-02, -1.4229e-02, -3.7503e-03, -1.6755e-02,\n",
      "        -3.3837e-02,  8.4429e-04, -1.6732e-02,  1.4206e-02,  3.8338e-02,\n",
      "        -4.3839e-02,  4.1797e-02, -5.5094e-03, -6.2061e-03, -2.3521e-02,\n",
      "         4.5979e-03,  2.4585e-02,  3.3642e-02, -1.6227e-02, -3.9078e-02,\n",
      "         9.2448e-03,  3.2480e-02, -3.3833e-02, -3.0530e-02, -1.0285e-02,\n",
      "         1.5209e-02,  2.4945e-02, -4.0465e-02,  2.0393e-02,  3.3612e-02,\n",
      "         1.8303e-02,  3.5432e-02, -2.5107e-02, -3.8605e-02,  3.3089e-02,\n",
      "        -1.1357e-02,  1.6814e-02,  1.8179e-02,  2.8897e-02,  2.4145e-02,\n",
      "        -2.4991e-02,  3.4318e-02,  2.6490e-03, -3.1062e-03, -2.7604e-02,\n",
      "        -2.8532e-02, -3.5882e-02, -3.4305e-02,  2.0562e-02, -3.8199e-02,\n",
      "         3.4368e-02, -4.0793e-02, -1.8868e-02, -3.2391e-02,  4.1704e-02,\n",
      "        -3.6942e-02,  6.7570e-03,  3.4072e-02,  1.5444e-02, -1.5261e-02,\n",
      "         6.1287e-04,  4.2568e-02, -1.5210e-05,  2.6328e-03, -3.3238e-02,\n",
      "         3.5554e-02, -4.3524e-02,  1.8110e-02,  1.4620e-02, -4.8033e-03,\n",
      "        -4.2703e-02, -3.4706e-02,  2.7294e-02,  1.4558e-02, -2.7939e-03,\n",
      "        -4.1991e-02,  4.9880e-03,  3.9363e-02,  3.5814e-02, -5.5837e-03,\n",
      "        -2.1918e-02, -4.2327e-02,  4.0988e-02,  3.6628e-02,  3.1711e-02,\n",
      "        -2.2932e-02, -1.1411e-02,  3.0305e-02,  4.0676e-02, -4.2113e-02,\n",
      "         3.5690e-02,  2.3675e-02, -7.4196e-03,  1.1984e-03, -2.3802e-02,\n",
      "        -2.7454e-03, -2.3287e-02,  3.3877e-02,  3.6296e-02,  1.9953e-02,\n",
      "         3.6043e-02,  4.0790e-02, -1.8558e-02,  1.5234e-02, -3.0437e-02,\n",
      "        -1.2801e-02, -2.3132e-02,  3.3120e-02,  2.2546e-02,  3.1713e-02,\n",
      "         1.7385e-02, -1.1428e-02,  2.8073e-02, -2.6950e-02,  3.2020e-02,\n",
      "         1.1859e-02, -2.6932e-02, -3.5893e-02,  2.9108e-02,  2.6417e-02,\n",
      "        -2.0719e-02,  4.2283e-02,  1.3595e-02, -3.3187e-02,  6.1217e-03,\n",
      "         3.8334e-02,  2.3276e-02, -3.0639e-04,  1.9900e-02, -4.1718e-02,\n",
      "         2.8146e-02,  3.1520e-02, -2.7148e-02,  4.3000e-02, -3.6932e-02,\n",
      "         2.3967e-02,  2.3582e-02, -1.6983e-03,  1.7530e-02, -2.9018e-02,\n",
      "        -1.7451e-04,  1.1383e-02,  1.9087e-02,  3.4938e-02, -4.2927e-02,\n",
      "        -4.2308e-02, -1.5338e-02,  5.9340e-03,  1.2554e-02,  1.2212e-02,\n",
      "        -4.2226e-02, -1.2027e-02, -3.2215e-02,  2.4108e-02,  4.0178e-02,\n",
      "         2.2338e-02,  3.4453e-02, -3.0739e-02, -2.0730e-02,  1.7357e-02,\n",
      "        -2.0594e-03, -6.1240e-03, -7.0556e-03,  4.4575e-03,  2.2920e-02,\n",
      "         3.8154e-02,  3.7522e-02, -3.9625e-02, -3.0646e-02,  2.7891e-02,\n",
      "         2.4521e-02, -2.5352e-02,  2.8836e-02, -4.3465e-02,  4.2007e-02,\n",
      "        -3.2525e-02, -2.4830e-02,  3.3696e-02, -1.5904e-02,  7.5806e-03,\n",
      "        -2.9725e-03,  3.6212e-02,  2.4498e-02, -2.7687e-02, -3.2676e-02,\n",
      "         8.0745e-04, -2.3802e-02, -3.7977e-02,  1.5621e-02, -2.3175e-02,\n",
      "         1.0271e-02,  1.5053e-02], device='mps:0', requires_grad=True)\n",
      "name: linear_relu_stack.4.weight, parameter: Parameter containing:\n",
      "tensor([[-0.0172,  0.0235,  0.0185,  ..., -0.0008, -0.0420, -0.0160],\n",
      "        [ 0.0142,  0.0226,  0.0330,  ..., -0.0257, -0.0170,  0.0116],\n",
      "        [ 0.0230,  0.0210,  0.0402,  ...,  0.0182,  0.0283,  0.0298],\n",
      "        ...,\n",
      "        [ 0.0042,  0.0036, -0.0250,  ..., -0.0374, -0.0401,  0.0352],\n",
      "        [-0.0223,  0.0415,  0.0218,  ..., -0.0345,  0.0041,  0.0068],\n",
      "        [-0.0408,  0.0273, -0.0237,  ..., -0.0272, -0.0221, -0.0252]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "name: linear_relu_stack.4.bias, parameter: Parameter containing:\n",
      "tensor([-0.0046,  0.0291,  0.0383, -0.0244,  0.0344, -0.0056,  0.0142, -0.0299,\n",
      "        -0.0051, -0.0333], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    print(\"name: {}, parameter: {}\".format(name, parameter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
