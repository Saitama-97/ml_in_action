{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习PyTorch-Day5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型是一个迭代过程：在每次迭代中，模型都会对输出进行猜测，计算其猜测的误差（损失），收集误差相对于其参数的导数，并使用梯度下降优化这些参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(dataset=training_data, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=16, shuffle=True)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 超参数是一堆可以控制整个优化过程的可调整参数\n",
    "- 不同的超参数可以对训练过程及模型收敛造成影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Number of Epochs__：在数据集上迭代的次数\n",
    "2. __Batch Size__：参数更新之前通过网络传播的数据样本的数量\n",
    "3. __Learning Rate__：每个 __batch/epoch__ 更新模型参数的数量。较小的学习率会导致学习速度较慢，而较大的学习率可能会导致训练期间出现不可预测的行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 16\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 回归任务 —— __nn.MSELoss__ (Mean Square Error)\n",
    "- 分类任务 —— __nn.NLLLoss__ (Negative Log Likelihood)\n",
    "- __nn.CrossEntropyLoss__ 包括了 __nn.LogSoftMax__ 和 __nn.NLLLoss__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 所有的优化逻辑都被封装在 __optimizer__ 对象中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初始化需要传入的参数\n",
    "1. 整个模型的网络参数 __model.parameters( )__\n",
    "2. 学习率 __learning_rate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 调用 __optimizer.zero_grad( )__ 来重置所有模型参数的梯度\n",
    "2. 通过调用 __loss.backward( )__ 反向传播预测损失\n",
    "3. 一旦我们得到了梯度，我们就调用 __optimizer.step( )__ 来调整参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具体实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "        训练过程\n",
    "    Args:\n",
    "        dataloader (_type_): _description_\n",
    "        model (_type_): _description_\n",
    "        loss_fn (_type_): _description_\n",
    "        optimizer (_type_): _description_\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 预测\n",
    "        pred = model(X.to(device))\n",
    "        # 基于预测结果和标签计算 loss\n",
    "        loss = loss_fn(pred, y.to(device))\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 优化参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 重置梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "        测试过程\n",
    "    Args:\n",
    "        dataloader (_type_): _description_\n",
    "        model (_type_): _description_\n",
    "        loss_fn (_type_): _description_\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    print(\"size={}\".format(size))\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    print(\"num_batches={}\".format(num_batches))\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.to(device))\n",
    "            test_loss += loss_fn(pred, y.to(device)).item()\n",
    "            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.313297  [   16/60000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.293334  [ 1616/60000]\n",
      "loss: 2.273076  [ 3216/60000]\n",
      "loss: 2.272763  [ 4816/60000]\n",
      "loss: 2.246865  [ 6416/60000]\n",
      "loss: 2.231766  [ 8016/60000]\n",
      "loss: 2.246040  [ 9616/60000]\n",
      "loss: 2.241057  [11216/60000]\n",
      "loss: 2.211491  [12816/60000]\n",
      "loss: 2.189331  [14416/60000]\n",
      "loss: 2.168194  [16016/60000]\n",
      "loss: 2.086901  [17616/60000]\n",
      "loss: 2.108777  [19216/60000]\n",
      "loss: 2.118653  [20816/60000]\n",
      "loss: 2.128221  [22416/60000]\n",
      "loss: 2.057017  [24016/60000]\n",
      "loss: 1.918358  [25616/60000]\n",
      "loss: 1.970721  [27216/60000]\n",
      "loss: 1.949620  [28816/60000]\n",
      "loss: 1.971829  [30416/60000]\n",
      "loss: 1.893253  [32016/60000]\n",
      "loss: 1.800910  [33616/60000]\n",
      "loss: 1.838858  [35216/60000]\n",
      "loss: 1.568265  [36816/60000]\n",
      "loss: 1.627336  [38416/60000]\n",
      "loss: 1.675721  [40016/60000]\n",
      "loss: 1.756574  [41616/60000]\n",
      "loss: 1.570633  [43216/60000]\n",
      "loss: 1.742628  [44816/60000]\n",
      "loss: 1.485454  [46416/60000]\n",
      "loss: 1.246074  [48016/60000]\n",
      "loss: 1.412761  [49616/60000]\n",
      "loss: 1.393174  [51216/60000]\n",
      "loss: 1.289234  [52816/60000]\n",
      "loss: 1.260654  [54416/60000]\n",
      "loss: 1.303025  [56016/60000]\n",
      "loss: 1.379466  [57616/60000]\n",
      "loss: 1.204918  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.297907 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.258371  [   16/60000]\n",
      "loss: 1.422835  [ 1616/60000]\n",
      "loss: 1.272038  [ 3216/60000]\n",
      "loss: 1.202659  [ 4816/60000]\n",
      "loss: 0.961663  [ 6416/60000]\n",
      "loss: 1.176715  [ 8016/60000]\n",
      "loss: 1.207890  [ 9616/60000]\n",
      "loss: 1.089756  [11216/60000]\n",
      "loss: 1.122817  [12816/60000]\n",
      "loss: 0.974778  [14416/60000]\n",
      "loss: 1.152031  [16016/60000]\n",
      "loss: 1.122324  [17616/60000]\n",
      "loss: 1.263253  [19216/60000]\n",
      "loss: 1.136933  [20816/60000]\n",
      "loss: 1.113219  [22416/60000]\n",
      "loss: 1.084885  [24016/60000]\n",
      "loss: 1.137545  [25616/60000]\n",
      "loss: 1.006273  [27216/60000]\n",
      "loss: 1.067290  [28816/60000]\n",
      "loss: 0.918835  [30416/60000]\n",
      "loss: 0.786207  [32016/60000]\n",
      "loss: 0.778895  [33616/60000]\n",
      "loss: 0.940080  [35216/60000]\n",
      "loss: 0.839951  [36816/60000]\n",
      "loss: 0.904502  [38416/60000]\n",
      "loss: 1.049573  [40016/60000]\n",
      "loss: 0.995495  [41616/60000]\n",
      "loss: 0.916145  [43216/60000]\n",
      "loss: 0.963831  [44816/60000]\n",
      "loss: 0.756729  [46416/60000]\n",
      "loss: 0.861028  [48016/60000]\n",
      "loss: 0.904739  [49616/60000]\n",
      "loss: 0.739323  [51216/60000]\n",
      "loss: 0.816052  [52816/60000]\n",
      "loss: 1.164699  [54416/60000]\n",
      "loss: 0.803788  [56016/60000]\n",
      "loss: 0.777702  [57616/60000]\n",
      "loss: 0.637560  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.873881 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.949660  [   16/60000]\n",
      "loss: 0.833341  [ 1616/60000]\n",
      "loss: 0.890317  [ 3216/60000]\n",
      "loss: 0.729443  [ 4816/60000]\n",
      "loss: 0.982949  [ 6416/60000]\n",
      "loss: 0.992794  [ 8016/60000]\n",
      "loss: 0.856510  [ 9616/60000]\n",
      "loss: 0.840299  [11216/60000]\n",
      "loss: 0.678618  [12816/60000]\n",
      "loss: 1.005160  [14416/60000]\n",
      "loss: 0.763681  [16016/60000]\n",
      "loss: 0.612475  [17616/60000]\n",
      "loss: 0.566986  [19216/60000]\n",
      "loss: 0.856971  [20816/60000]\n",
      "loss: 0.655116  [22416/60000]\n",
      "loss: 0.814288  [24016/60000]\n",
      "loss: 0.708803  [25616/60000]\n",
      "loss: 0.712990  [27216/60000]\n",
      "loss: 0.830972  [28816/60000]\n",
      "loss: 0.450749  [30416/60000]\n",
      "loss: 0.894539  [32016/60000]\n",
      "loss: 0.538262  [33616/60000]\n",
      "loss: 0.857582  [35216/60000]\n",
      "loss: 0.843265  [36816/60000]\n",
      "loss: 0.502918  [38416/60000]\n",
      "loss: 0.765679  [40016/60000]\n",
      "loss: 0.833767  [41616/60000]\n",
      "loss: 0.866483  [43216/60000]\n",
      "loss: 0.808988  [44816/60000]\n",
      "loss: 0.485618  [46416/60000]\n",
      "loss: 0.752524  [48016/60000]\n",
      "loss: 1.295846  [49616/60000]\n",
      "loss: 0.598897  [51216/60000]\n",
      "loss: 0.943425  [52816/60000]\n",
      "loss: 0.835456  [54416/60000]\n",
      "loss: 0.691711  [56016/60000]\n",
      "loss: 0.881836  [57616/60000]\n",
      "loss: 0.981637  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.751562 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.682640  [   16/60000]\n",
      "loss: 0.533287  [ 1616/60000]\n",
      "loss: 0.681809  [ 3216/60000]\n",
      "loss: 1.176960  [ 4816/60000]\n",
      "loss: 0.591000  [ 6416/60000]\n",
      "loss: 0.506792  [ 8016/60000]\n",
      "loss: 0.981416  [ 9616/60000]\n",
      "loss: 0.598668  [11216/60000]\n",
      "loss: 0.567791  [12816/60000]\n",
      "loss: 1.104134  [14416/60000]\n",
      "loss: 0.723093  [16016/60000]\n",
      "loss: 0.751728  [17616/60000]\n",
      "loss: 0.483324  [19216/60000]\n",
      "loss: 0.507147  [20816/60000]\n",
      "loss: 0.705657  [22416/60000]\n",
      "loss: 0.794352  [24016/60000]\n",
      "loss: 1.256524  [25616/60000]\n",
      "loss: 0.529063  [27216/60000]\n",
      "loss: 0.936666  [28816/60000]\n",
      "loss: 0.544904  [30416/60000]\n",
      "loss: 0.373567  [32016/60000]\n",
      "loss: 0.819758  [33616/60000]\n",
      "loss: 0.850450  [35216/60000]\n",
      "loss: 0.790350  [36816/60000]\n",
      "loss: 0.451419  [38416/60000]\n",
      "loss: 0.862337  [40016/60000]\n",
      "loss: 0.547052  [41616/60000]\n",
      "loss: 0.784155  [43216/60000]\n",
      "loss: 0.782601  [44816/60000]\n",
      "loss: 0.539446  [46416/60000]\n",
      "loss: 0.691183  [48016/60000]\n",
      "loss: 0.749406  [49616/60000]\n",
      "loss: 0.647291  [51216/60000]\n",
      "loss: 0.617451  [52816/60000]\n",
      "loss: 0.811526  [54416/60000]\n",
      "loss: 0.543737  [56016/60000]\n",
      "loss: 0.462044  [57616/60000]\n",
      "loss: 0.595860  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.678991 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.541121  [   16/60000]\n",
      "loss: 0.865050  [ 1616/60000]\n",
      "loss: 0.882498  [ 3216/60000]\n",
      "loss: 0.769096  [ 4816/60000]\n",
      "loss: 0.646520  [ 6416/60000]\n",
      "loss: 0.637653  [ 8016/60000]\n",
      "loss: 0.749933  [ 9616/60000]\n",
      "loss: 0.605547  [11216/60000]\n",
      "loss: 1.179612  [12816/60000]\n",
      "loss: 0.489684  [14416/60000]\n",
      "loss: 0.457797  [16016/60000]\n",
      "loss: 0.535938  [17616/60000]\n",
      "loss: 0.717115  [19216/60000]\n",
      "loss: 0.700093  [20816/60000]\n",
      "loss: 0.952926  [22416/60000]\n",
      "loss: 0.505392  [24016/60000]\n",
      "loss: 0.482104  [25616/60000]\n",
      "loss: 0.717956  [27216/60000]\n",
      "loss: 0.607181  [28816/60000]\n",
      "loss: 0.678970  [30416/60000]\n",
      "loss: 0.552583  [32016/60000]\n",
      "loss: 0.679945  [33616/60000]\n",
      "loss: 0.437167  [35216/60000]\n",
      "loss: 0.550389  [36816/60000]\n",
      "loss: 0.610577  [38416/60000]\n",
      "loss: 0.523355  [40016/60000]\n",
      "loss: 0.582904  [41616/60000]\n",
      "loss: 0.502408  [43216/60000]\n",
      "loss: 0.761161  [44816/60000]\n",
      "loss: 0.430642  [46416/60000]\n",
      "loss: 0.249414  [48016/60000]\n",
      "loss: 0.863877  [49616/60000]\n",
      "loss: 0.675961  [51216/60000]\n",
      "loss: 0.582857  [52816/60000]\n",
      "loss: 0.735203  [54416/60000]\n",
      "loss: 1.191073  [56016/60000]\n",
      "loss: 0.654506  [57616/60000]\n",
      "loss: 0.549780  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.629195 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.817807  [   16/60000]\n",
      "loss: 0.404666  [ 1616/60000]\n",
      "loss: 0.294576  [ 3216/60000]\n",
      "loss: 0.490997  [ 4816/60000]\n",
      "loss: 0.363878  [ 6416/60000]\n",
      "loss: 0.652397  [ 8016/60000]\n",
      "loss: 1.061491  [ 9616/60000]\n",
      "loss: 0.614443  [11216/60000]\n",
      "loss: 0.861596  [12816/60000]\n",
      "loss: 0.577012  [14416/60000]\n",
      "loss: 0.505423  [16016/60000]\n",
      "loss: 0.516141  [17616/60000]\n",
      "loss: 0.326427  [19216/60000]\n",
      "loss: 0.543589  [20816/60000]\n",
      "loss: 0.578014  [22416/60000]\n",
      "loss: 0.823194  [24016/60000]\n",
      "loss: 0.571049  [25616/60000]\n",
      "loss: 0.614340  [27216/60000]\n",
      "loss: 0.465455  [28816/60000]\n",
      "loss: 0.707861  [30416/60000]\n",
      "loss: 0.739337  [32016/60000]\n",
      "loss: 0.507654  [33616/60000]\n",
      "loss: 0.660940  [35216/60000]\n",
      "loss: 0.673069  [36816/60000]\n",
      "loss: 0.817498  [38416/60000]\n",
      "loss: 0.480415  [40016/60000]\n",
      "loss: 0.371750  [41616/60000]\n",
      "loss: 0.741030  [43216/60000]\n",
      "loss: 0.398658  [44816/60000]\n",
      "loss: 0.700734  [46416/60000]\n",
      "loss: 0.307391  [48016/60000]\n",
      "loss: 0.805606  [49616/60000]\n",
      "loss: 0.671836  [51216/60000]\n",
      "loss: 0.752616  [52816/60000]\n",
      "loss: 0.510428  [54416/60000]\n",
      "loss: 0.580204  [56016/60000]\n",
      "loss: 0.510675  [57616/60000]\n",
      "loss: 0.688717  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.594456 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.607441  [   16/60000]\n",
      "loss: 0.665952  [ 1616/60000]\n",
      "loss: 0.351303  [ 3216/60000]\n",
      "loss: 0.768286  [ 4816/60000]\n",
      "loss: 0.294564  [ 6416/60000]\n",
      "loss: 0.329710  [ 8016/60000]\n",
      "loss: 0.444267  [ 9616/60000]\n",
      "loss: 0.936640  [11216/60000]\n",
      "loss: 0.858454  [12816/60000]\n",
      "loss: 0.428400  [14416/60000]\n",
      "loss: 0.406906  [16016/60000]\n",
      "loss: 0.335364  [17616/60000]\n",
      "loss: 0.950564  [19216/60000]\n",
      "loss: 0.604586  [20816/60000]\n",
      "loss: 0.617800  [22416/60000]\n",
      "loss: 1.240296  [24016/60000]\n",
      "loss: 0.513036  [25616/60000]\n",
      "loss: 0.550530  [27216/60000]\n",
      "loss: 0.488725  [28816/60000]\n",
      "loss: 0.545812  [30416/60000]\n",
      "loss: 0.367922  [32016/60000]\n",
      "loss: 0.856658  [33616/60000]\n",
      "loss: 0.412773  [35216/60000]\n",
      "loss: 0.294305  [36816/60000]\n",
      "loss: 0.366705  [38416/60000]\n",
      "loss: 0.467430  [40016/60000]\n",
      "loss: 0.706769  [41616/60000]\n",
      "loss: 0.886183  [43216/60000]\n",
      "loss: 0.373164  [44816/60000]\n",
      "loss: 0.507781  [46416/60000]\n",
      "loss: 0.368584  [48016/60000]\n",
      "loss: 0.540358  [49616/60000]\n",
      "loss: 0.618053  [51216/60000]\n",
      "loss: 0.654055  [52816/60000]\n",
      "loss: 0.468489  [54416/60000]\n",
      "loss: 0.492592  [56016/60000]\n",
      "loss: 0.533824  [57616/60000]\n",
      "loss: 0.589394  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.565981 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.645200  [   16/60000]\n",
      "loss: 0.767016  [ 1616/60000]\n",
      "loss: 0.576675  [ 3216/60000]\n",
      "loss: 0.766265  [ 4816/60000]\n",
      "loss: 0.923110  [ 6416/60000]\n",
      "loss: 0.267782  [ 8016/60000]\n",
      "loss: 0.462892  [ 9616/60000]\n",
      "loss: 0.380213  [11216/60000]\n",
      "loss: 0.413834  [12816/60000]\n",
      "loss: 0.277104  [14416/60000]\n",
      "loss: 0.670248  [16016/60000]\n",
      "loss: 0.398067  [17616/60000]\n",
      "loss: 0.563989  [19216/60000]\n",
      "loss: 0.561747  [20816/60000]\n",
      "loss: 0.649179  [22416/60000]\n",
      "loss: 0.454459  [24016/60000]\n",
      "loss: 0.624404  [25616/60000]\n",
      "loss: 0.441499  [27216/60000]\n",
      "loss: 0.402204  [28816/60000]\n",
      "loss: 0.375098  [30416/60000]\n",
      "loss: 0.474819  [32016/60000]\n",
      "loss: 0.522809  [33616/60000]\n",
      "loss: 0.374580  [35216/60000]\n",
      "loss: 0.635495  [36816/60000]\n",
      "loss: 0.561489  [38416/60000]\n",
      "loss: 0.886947  [40016/60000]\n",
      "loss: 0.753766  [41616/60000]\n",
      "loss: 0.642689  [43216/60000]\n",
      "loss: 0.398332  [44816/60000]\n",
      "loss: 0.561837  [46416/60000]\n",
      "loss: 0.424370  [48016/60000]\n",
      "loss: 0.301404  [49616/60000]\n",
      "loss: 0.550546  [51216/60000]\n",
      "loss: 0.531571  [52816/60000]\n",
      "loss: 0.385830  [54416/60000]\n",
      "loss: 0.482929  [56016/60000]\n",
      "loss: 0.575950  [57616/60000]\n",
      "loss: 0.899753  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.545895 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.540085  [   16/60000]\n",
      "loss: 0.548983  [ 1616/60000]\n",
      "loss: 0.343667  [ 3216/60000]\n",
      "loss: 0.476826  [ 4816/60000]\n",
      "loss: 0.754200  [ 6416/60000]\n",
      "loss: 0.698079  [ 8016/60000]\n",
      "loss: 0.327776  [ 9616/60000]\n",
      "loss: 0.392126  [11216/60000]\n",
      "loss: 0.688532  [12816/60000]\n",
      "loss: 0.357786  [14416/60000]\n",
      "loss: 0.418061  [16016/60000]\n",
      "loss: 0.787718  [17616/60000]\n",
      "loss: 0.587363  [19216/60000]\n",
      "loss: 0.322169  [20816/60000]\n",
      "loss: 1.391649  [22416/60000]\n",
      "loss: 0.746722  [24016/60000]\n",
      "loss: 0.490625  [25616/60000]\n",
      "loss: 0.456712  [27216/60000]\n",
      "loss: 0.447484  [28816/60000]\n",
      "loss: 0.571488  [30416/60000]\n",
      "loss: 0.251334  [32016/60000]\n",
      "loss: 0.444011  [33616/60000]\n",
      "loss: 0.691670  [35216/60000]\n",
      "loss: 0.574338  [36816/60000]\n",
      "loss: 0.259863  [38416/60000]\n",
      "loss: 0.433801  [40016/60000]\n",
      "loss: 0.666623  [41616/60000]\n",
      "loss: 0.574636  [43216/60000]\n",
      "loss: 0.610794  [44816/60000]\n",
      "loss: 0.465135  [46416/60000]\n",
      "loss: 0.655729  [48016/60000]\n",
      "loss: 0.445935  [49616/60000]\n",
      "loss: 0.211669  [51216/60000]\n",
      "loss: 0.491843  [52816/60000]\n",
      "loss: 1.120225  [54416/60000]\n",
      "loss: 0.588139  [56016/60000]\n",
      "loss: 0.249553  [57616/60000]\n",
      "loss: 0.566084  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.532376 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.637796  [   16/60000]\n",
      "loss: 0.351582  [ 1616/60000]\n",
      "loss: 0.413755  [ 3216/60000]\n",
      "loss: 0.428462  [ 4816/60000]\n",
      "loss: 0.492054  [ 6416/60000]\n",
      "loss: 0.640194  [ 8016/60000]\n",
      "loss: 0.481628  [ 9616/60000]\n",
      "loss: 0.434466  [11216/60000]\n",
      "loss: 0.245176  [12816/60000]\n",
      "loss: 0.562576  [14416/60000]\n",
      "loss: 0.703115  [16016/60000]\n",
      "loss: 0.692137  [17616/60000]\n",
      "loss: 0.637669  [19216/60000]\n",
      "loss: 0.667937  [20816/60000]\n",
      "loss: 0.407283  [22416/60000]\n",
      "loss: 0.495076  [24016/60000]\n",
      "loss: 0.484885  [25616/60000]\n",
      "loss: 0.407813  [27216/60000]\n",
      "loss: 0.323368  [28816/60000]\n",
      "loss: 0.827343  [30416/60000]\n",
      "loss: 0.519805  [32016/60000]\n",
      "loss: 0.534465  [33616/60000]\n",
      "loss: 0.710024  [35216/60000]\n",
      "loss: 0.609375  [36816/60000]\n",
      "loss: 0.466425  [38416/60000]\n",
      "loss: 0.554715  [40016/60000]\n",
      "loss: 0.245527  [41616/60000]\n",
      "loss: 0.415482  [43216/60000]\n",
      "loss: 0.572620  [44816/60000]\n",
      "loss: 0.194685  [46416/60000]\n",
      "loss: 0.425346  [48016/60000]\n",
      "loss: 0.824563  [49616/60000]\n",
      "loss: 0.693982  [51216/60000]\n",
      "loss: 0.561905  [52816/60000]\n",
      "loss: 0.411988  [54416/60000]\n",
      "loss: 0.321049  [56016/60000]\n",
      "loss: 0.403736  [57616/60000]\n",
      "loss: 0.273551  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.522397 \n",
      "\n",
      "All Done in 358.42734813690186 secs !!!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(dataloader=train_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    test_loop(dataloader=test_dataloader, model=model, loss_fn=loss_fn)\n",
    "    \n",
    "t2 = time.time()\n",
    "\n",
    "print(\"All Done in {} secs !!!\".format(t2 - t1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
