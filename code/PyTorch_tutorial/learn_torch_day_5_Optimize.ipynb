{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习PyTorch-Day5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型是一个迭代过程：在每次迭代中，模型都会对输出进行猜测，计算其猜测的误差（损失），收集误差相对于其参数的导数，并使用梯度下降优化这些参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(dataset=training_data, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=16, shuffle=True)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 超参数是一堆可以控制整个优化过程的可调整参数\n",
    "- 不同的超参数可以对训练过程及模型收敛造成影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Number of Epochs__：在数据集上迭代的次数\n",
    "2. __Batch Size__：参数更新之前通过网络传播的数据样本的数量\n",
    "3. __Learning Rate__：每个 __batch/epoch__ 更新模型参数的数量。较小的学习率会导致学习速度较慢，而较大的学习率可能会导致训练期间出现不可预测的行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 16\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 回归任务 —— __nn.MSELoss__ (Mean Square Error)\n",
    "- 分类任务 —— __nn.NLLLoss__ (Negative Log Likelihood)\n",
    "- __nn.CrossEntropyLoss__ 包括了 __nn.LogSoftMax__ 和 __nn.NLLLoss__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 所有的优化逻辑都被封装在 __optimizer__ 对象中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初始化需要传入的参数\n",
    "1. 整个模型的网络参数 __model.parameters( )__\n",
    "2. 学习率 __learning_rate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 调用 __optimizer.zero_grad( )__ 来重置所有模型参数的梯度\n",
    "2. 通过调用 __loss.backward( )__ 反向传播预测损失\n",
    "3. 一旦我们得到了梯度，我们就调用 __optimizer.step( )__ 来调整参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具体实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "        训练过程\n",
    "    Args:\n",
    "        dataloader (_type_): _description_\n",
    "        model (_type_): _description_\n",
    "        loss_fn (_type_): _description_\n",
    "        optimizer (_type_): _description_\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 预测\n",
    "        pred = model(X)\n",
    "        # 基于预测结果和标签计算 loss\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 优化参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 重置梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "        测试过程\n",
    "    Args:\n",
    "        dataloader (_type_): _description_\n",
    "        model (_type_): _description_\n",
    "        loss_fn (_type_): _description_\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    print(\"size={}\".format(size))\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    print(\"num_batches={}\".format(num_batches))\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.307726  [   16/60000]\n",
      "loss: 2.287292  [ 1616/60000]\n",
      "loss: 2.257347  [ 3216/60000]\n",
      "loss: 2.229270  [ 4816/60000]\n",
      "loss: 2.255247  [ 6416/60000]\n",
      "loss: 2.211401  [ 8016/60000]\n",
      "loss: 2.253152  [ 9616/60000]\n",
      "loss: 2.197626  [11216/60000]\n",
      "loss: 2.145247  [12816/60000]\n",
      "loss: 2.148913  [14416/60000]\n",
      "loss: 2.168693  [16016/60000]\n",
      "loss: 2.172954  [17616/60000]\n",
      "loss: 2.079718  [19216/60000]\n",
      "loss: 2.075531  [20816/60000]\n",
      "loss: 1.967690  [22416/60000]\n",
      "loss: 2.015974  [24016/60000]\n",
      "loss: 1.965407  [25616/60000]\n",
      "loss: 1.948760  [27216/60000]\n",
      "loss: 1.940817  [28816/60000]\n",
      "loss: 1.836054  [30416/60000]\n",
      "loss: 1.806493  [32016/60000]\n",
      "loss: 1.729509  [33616/60000]\n",
      "loss: 1.848402  [35216/60000]\n",
      "loss: 1.861629  [36816/60000]\n",
      "loss: 1.637897  [38416/60000]\n",
      "loss: 1.537511  [40016/60000]\n",
      "loss: 1.779000  [41616/60000]\n",
      "loss: 1.750628  [43216/60000]\n",
      "loss: 1.434341  [44816/60000]\n",
      "loss: 1.568382  [46416/60000]\n",
      "loss: 1.355406  [48016/60000]\n",
      "loss: 1.563740  [49616/60000]\n",
      "loss: 1.327588  [51216/60000]\n",
      "loss: 1.657354  [52816/60000]\n",
      "loss: 1.313454  [54416/60000]\n",
      "loss: 1.251356  [56016/60000]\n",
      "loss: 1.092010  [57616/60000]\n",
      "loss: 1.212434  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.239075 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.196579  [   16/60000]\n",
      "loss: 1.562685  [ 1616/60000]\n",
      "loss: 1.336768  [ 3216/60000]\n",
      "loss: 1.272495  [ 4816/60000]\n",
      "loss: 1.279372  [ 6416/60000]\n",
      "loss: 0.888949  [ 8016/60000]\n",
      "loss: 1.185655  [ 9616/60000]\n",
      "loss: 0.891749  [11216/60000]\n",
      "loss: 1.186706  [12816/60000]\n",
      "loss: 1.006145  [14416/60000]\n",
      "loss: 0.880065  [16016/60000]\n",
      "loss: 1.100305  [17616/60000]\n",
      "loss: 1.297194  [19216/60000]\n",
      "loss: 1.076588  [20816/60000]\n",
      "loss: 0.906555  [22416/60000]\n",
      "loss: 1.128037  [24016/60000]\n",
      "loss: 0.750733  [25616/60000]\n",
      "loss: 1.077799  [27216/60000]\n",
      "loss: 0.936487  [28816/60000]\n",
      "loss: 1.017365  [30416/60000]\n",
      "loss: 0.864721  [32016/60000]\n",
      "loss: 1.140403  [33616/60000]\n",
      "loss: 0.979375  [35216/60000]\n",
      "loss: 1.122205  [36816/60000]\n",
      "loss: 0.959462  [38416/60000]\n",
      "loss: 0.919786  [40016/60000]\n",
      "loss: 0.726889  [41616/60000]\n",
      "loss: 1.102119  [43216/60000]\n",
      "loss: 1.221677  [44816/60000]\n",
      "loss: 0.990744  [46416/60000]\n",
      "loss: 0.753112  [48016/60000]\n",
      "loss: 0.826277  [49616/60000]\n",
      "loss: 0.722178  [51216/60000]\n",
      "loss: 0.887818  [52816/60000]\n",
      "loss: 0.867127  [54416/60000]\n",
      "loss: 0.956419  [56016/60000]\n",
      "loss: 0.934932  [57616/60000]\n",
      "loss: 0.721261  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.850391 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.752301  [   16/60000]\n",
      "loss: 0.613143  [ 1616/60000]\n",
      "loss: 0.953947  [ 3216/60000]\n",
      "loss: 0.966929  [ 4816/60000]\n",
      "loss: 0.955473  [ 6416/60000]\n",
      "loss: 0.841747  [ 8016/60000]\n",
      "loss: 0.774317  [ 9616/60000]\n",
      "loss: 0.942292  [11216/60000]\n",
      "loss: 0.797835  [12816/60000]\n",
      "loss: 0.809472  [14416/60000]\n",
      "loss: 1.066767  [16016/60000]\n",
      "loss: 0.865539  [17616/60000]\n",
      "loss: 1.012545  [19216/60000]\n",
      "loss: 0.817022  [20816/60000]\n",
      "loss: 1.073102  [22416/60000]\n",
      "loss: 0.587780  [24016/60000]\n",
      "loss: 0.838707  [25616/60000]\n",
      "loss: 0.679269  [27216/60000]\n",
      "loss: 0.576820  [28816/60000]\n",
      "loss: 0.954773  [30416/60000]\n",
      "loss: 0.844312  [32016/60000]\n",
      "loss: 0.886312  [33616/60000]\n",
      "loss: 0.569478  [35216/60000]\n",
      "loss: 0.619449  [36816/60000]\n",
      "loss: 0.738409  [38416/60000]\n",
      "loss: 0.678533  [40016/60000]\n",
      "loss: 0.951381  [41616/60000]\n",
      "loss: 0.512834  [43216/60000]\n",
      "loss: 0.529338  [44816/60000]\n",
      "loss: 0.716687  [46416/60000]\n",
      "loss: 0.516945  [48016/60000]\n",
      "loss: 0.431905  [49616/60000]\n",
      "loss: 1.090836  [51216/60000]\n",
      "loss: 0.538007  [52816/60000]\n",
      "loss: 1.074454  [54416/60000]\n",
      "loss: 0.690303  [56016/60000]\n",
      "loss: 0.621305  [57616/60000]\n",
      "loss: 0.459918  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.739290 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.619673  [   16/60000]\n",
      "loss: 0.922753  [ 1616/60000]\n",
      "loss: 0.479311  [ 3216/60000]\n",
      "loss: 0.728070  [ 4816/60000]\n",
      "loss: 0.630576  [ 6416/60000]\n",
      "loss: 0.799163  [ 8016/60000]\n",
      "loss: 0.691092  [ 9616/60000]\n",
      "loss: 0.863020  [11216/60000]\n",
      "loss: 0.597021  [12816/60000]\n",
      "loss: 0.485096  [14416/60000]\n",
      "loss: 0.444891  [16016/60000]\n",
      "loss: 0.710886  [17616/60000]\n",
      "loss: 0.836243  [19216/60000]\n",
      "loss: 1.051676  [20816/60000]\n",
      "loss: 0.979443  [22416/60000]\n",
      "loss: 0.767391  [24016/60000]\n",
      "loss: 1.101601  [25616/60000]\n",
      "loss: 0.677565  [27216/60000]\n",
      "loss: 0.558566  [28816/60000]\n",
      "loss: 0.834180  [30416/60000]\n",
      "loss: 0.480715  [32016/60000]\n",
      "loss: 1.030433  [33616/60000]\n",
      "loss: 0.485658  [35216/60000]\n",
      "loss: 0.693262  [36816/60000]\n",
      "loss: 0.619712  [38416/60000]\n",
      "loss: 0.877858  [40016/60000]\n",
      "loss: 1.334206  [41616/60000]\n",
      "loss: 0.736328  [43216/60000]\n",
      "loss: 0.801188  [44816/60000]\n",
      "loss: 0.971154  [46416/60000]\n",
      "loss: 0.856212  [48016/60000]\n",
      "loss: 0.540936  [49616/60000]\n",
      "loss: 0.230252  [51216/60000]\n",
      "loss: 0.612001  [52816/60000]\n",
      "loss: 0.683753  [54416/60000]\n",
      "loss: 0.661720  [56016/60000]\n",
      "loss: 0.720615  [57616/60000]\n",
      "loss: 0.620081  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.675126 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.763466  [   16/60000]\n",
      "loss: 0.636200  [ 1616/60000]\n",
      "loss: 0.280613  [ 3216/60000]\n",
      "loss: 0.650324  [ 4816/60000]\n",
      "loss: 0.628856  [ 6416/60000]\n",
      "loss: 0.458115  [ 8016/60000]\n",
      "loss: 0.401054  [ 9616/60000]\n",
      "loss: 0.381559  [11216/60000]\n",
      "loss: 0.280939  [12816/60000]\n",
      "loss: 0.517373  [14416/60000]\n",
      "loss: 0.400518  [16016/60000]\n",
      "loss: 0.575243  [17616/60000]\n",
      "loss: 0.705004  [19216/60000]\n",
      "loss: 0.659815  [20816/60000]\n",
      "loss: 0.705086  [22416/60000]\n",
      "loss: 1.055573  [24016/60000]\n",
      "loss: 0.428729  [25616/60000]\n",
      "loss: 0.767976  [27216/60000]\n",
      "loss: 0.398747  [28816/60000]\n",
      "loss: 0.436429  [30416/60000]\n",
      "loss: 0.499444  [32016/60000]\n",
      "loss: 0.876450  [33616/60000]\n",
      "loss: 0.884315  [35216/60000]\n",
      "loss: 0.852353  [36816/60000]\n",
      "loss: 0.401556  [38416/60000]\n",
      "loss: 0.507984  [40016/60000]\n",
      "loss: 0.596407  [41616/60000]\n",
      "loss: 0.495222  [43216/60000]\n",
      "loss: 0.561341  [44816/60000]\n",
      "loss: 0.564182  [46416/60000]\n",
      "loss: 0.497581  [48016/60000]\n",
      "loss: 0.826209  [49616/60000]\n",
      "loss: 0.495356  [51216/60000]\n",
      "loss: 0.573427  [52816/60000]\n",
      "loss: 0.590113  [54416/60000]\n",
      "loss: 0.463047  [56016/60000]\n",
      "loss: 0.773218  [57616/60000]\n",
      "loss: 0.691838  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.624452 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.010530  [   16/60000]\n",
      "loss: 0.531739  [ 1616/60000]\n",
      "loss: 0.788434  [ 3216/60000]\n",
      "loss: 0.647639  [ 4816/60000]\n",
      "loss: 0.607669  [ 6416/60000]\n",
      "loss: 0.371113  [ 8016/60000]\n",
      "loss: 0.434184  [ 9616/60000]\n",
      "loss: 0.297230  [11216/60000]\n",
      "loss: 0.471673  [12816/60000]\n",
      "loss: 0.461200  [14416/60000]\n",
      "loss: 0.555467  [16016/60000]\n",
      "loss: 0.497747  [17616/60000]\n",
      "loss: 0.557392  [19216/60000]\n",
      "loss: 0.519884  [20816/60000]\n",
      "loss: 0.465893  [22416/60000]\n",
      "loss: 0.594466  [24016/60000]\n",
      "loss: 0.708689  [25616/60000]\n",
      "loss: 0.589043  [27216/60000]\n",
      "loss: 0.505350  [28816/60000]\n",
      "loss: 0.486474  [30416/60000]\n",
      "loss: 1.181713  [32016/60000]\n",
      "loss: 0.438566  [33616/60000]\n",
      "loss: 0.581670  [35216/60000]\n",
      "loss: 0.655378  [36816/60000]\n",
      "loss: 0.764328  [38416/60000]\n",
      "loss: 0.420933  [40016/60000]\n",
      "loss: 1.191303  [41616/60000]\n",
      "loss: 0.418573  [43216/60000]\n",
      "loss: 0.419571  [44816/60000]\n",
      "loss: 0.552815  [46416/60000]\n",
      "loss: 0.477914  [48016/60000]\n",
      "loss: 0.571522  [49616/60000]\n",
      "loss: 0.197465  [51216/60000]\n",
      "loss: 0.463059  [52816/60000]\n",
      "loss: 0.305871  [54416/60000]\n",
      "loss: 0.804493  [56016/60000]\n",
      "loss: 0.912826  [57616/60000]\n",
      "loss: 0.576880  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.589751 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.634669  [   16/60000]\n",
      "loss: 0.582898  [ 1616/60000]\n",
      "loss: 0.839283  [ 3216/60000]\n",
      "loss: 0.309647  [ 4816/60000]\n",
      "loss: 0.618869  [ 6416/60000]\n",
      "loss: 0.808082  [ 8016/60000]\n",
      "loss: 0.458281  [ 9616/60000]\n",
      "loss: 0.237016  [11216/60000]\n",
      "loss: 0.400239  [12816/60000]\n",
      "loss: 0.334314  [14416/60000]\n",
      "loss: 0.511065  [16016/60000]\n",
      "loss: 0.423026  [17616/60000]\n",
      "loss: 0.753919  [19216/60000]\n",
      "loss: 0.595400  [20816/60000]\n",
      "loss: 1.330041  [22416/60000]\n",
      "loss: 0.759996  [24016/60000]\n",
      "loss: 0.308356  [25616/60000]\n",
      "loss: 0.542196  [27216/60000]\n",
      "loss: 1.143316  [28816/60000]\n",
      "loss: 0.448725  [30416/60000]\n",
      "loss: 0.713547  [32016/60000]\n",
      "loss: 0.889446  [33616/60000]\n",
      "loss: 0.477125  [35216/60000]\n",
      "loss: 0.667002  [36816/60000]\n",
      "loss: 0.558032  [38416/60000]\n",
      "loss: 0.446828  [40016/60000]\n",
      "loss: 0.600397  [41616/60000]\n",
      "loss: 0.696934  [43216/60000]\n",
      "loss: 0.294734  [44816/60000]\n",
      "loss: 0.608735  [46416/60000]\n",
      "loss: 0.541257  [48016/60000]\n",
      "loss: 0.697818  [49616/60000]\n",
      "loss: 0.772541  [51216/60000]\n",
      "loss: 0.180039  [52816/60000]\n",
      "loss: 0.560484  [54416/60000]\n",
      "loss: 0.341688  [56016/60000]\n",
      "loss: 0.391144  [57616/60000]\n",
      "loss: 0.678548  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.565251 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.500738  [   16/60000]\n",
      "loss: 0.842046  [ 1616/60000]\n",
      "loss: 0.810550  [ 3216/60000]\n",
      "loss: 0.583365  [ 4816/60000]\n",
      "loss: 0.308825  [ 6416/60000]\n",
      "loss: 0.513810  [ 8016/60000]\n",
      "loss: 0.154437  [ 9616/60000]\n",
      "loss: 0.416646  [11216/60000]\n",
      "loss: 0.469527  [12816/60000]\n",
      "loss: 0.507665  [14416/60000]\n",
      "loss: 0.267107  [16016/60000]\n",
      "loss: 0.477544  [17616/60000]\n",
      "loss: 0.550631  [19216/60000]\n",
      "loss: 0.409736  [20816/60000]\n",
      "loss: 0.519193  [22416/60000]\n",
      "loss: 0.551281  [24016/60000]\n",
      "loss: 0.466434  [25616/60000]\n",
      "loss: 0.391793  [27216/60000]\n",
      "loss: 0.383355  [28816/60000]\n",
      "loss: 0.872164  [30416/60000]\n",
      "loss: 0.738380  [32016/60000]\n",
      "loss: 1.062228  [33616/60000]\n",
      "loss: 0.565307  [35216/60000]\n",
      "loss: 0.451284  [36816/60000]\n",
      "loss: 0.288404  [38416/60000]\n",
      "loss: 0.649731  [40016/60000]\n",
      "loss: 1.094938  [41616/60000]\n",
      "loss: 0.512255  [43216/60000]\n",
      "loss: 0.247986  [44816/60000]\n",
      "loss: 0.230295  [46416/60000]\n",
      "loss: 0.525650  [48016/60000]\n",
      "loss: 0.542781  [49616/60000]\n",
      "loss: 0.549837  [51216/60000]\n",
      "loss: 0.947717  [52816/60000]\n",
      "loss: 0.543338  [54416/60000]\n",
      "loss: 0.785260  [56016/60000]\n",
      "loss: 0.528462  [57616/60000]\n",
      "loss: 0.489113  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.545024 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.831970  [   16/60000]\n",
      "loss: 0.508768  [ 1616/60000]\n",
      "loss: 0.791972  [ 3216/60000]\n",
      "loss: 0.708874  [ 4816/60000]\n",
      "loss: 0.851370  [ 6416/60000]\n",
      "loss: 0.497986  [ 8016/60000]\n",
      "loss: 0.328250  [ 9616/60000]\n",
      "loss: 0.614815  [11216/60000]\n",
      "loss: 0.669850  [12816/60000]\n",
      "loss: 0.668686  [14416/60000]\n",
      "loss: 0.554205  [16016/60000]\n",
      "loss: 0.762621  [17616/60000]\n",
      "loss: 0.548409  [19216/60000]\n",
      "loss: 0.273135  [20816/60000]\n",
      "loss: 0.410853  [22416/60000]\n",
      "loss: 0.557129  [24016/60000]\n",
      "loss: 0.728796  [25616/60000]\n",
      "loss: 0.775392  [27216/60000]\n",
      "loss: 0.477141  [28816/60000]\n",
      "loss: 0.815531  [30416/60000]\n",
      "loss: 0.213636  [32016/60000]\n",
      "loss: 0.403130  [33616/60000]\n",
      "loss: 0.411147  [35216/60000]\n",
      "loss: 0.439671  [36816/60000]\n",
      "loss: 0.359568  [38416/60000]\n",
      "loss: 0.422031  [40016/60000]\n",
      "loss: 1.024601  [41616/60000]\n",
      "loss: 0.212396  [43216/60000]\n",
      "loss: 0.444132  [44816/60000]\n",
      "loss: 0.488186  [46416/60000]\n",
      "loss: 1.154175  [48016/60000]\n",
      "loss: 0.577070  [49616/60000]\n",
      "loss: 0.460983  [51216/60000]\n",
      "loss: 0.475380  [52816/60000]\n",
      "loss: 0.808051  [54416/60000]\n",
      "loss: 0.285205  [56016/60000]\n",
      "loss: 0.325289  [57616/60000]\n",
      "loss: 0.528621  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.533868 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.438490  [   16/60000]\n",
      "loss: 0.287895  [ 1616/60000]\n",
      "loss: 0.357679  [ 3216/60000]\n",
      "loss: 0.594029  [ 4816/60000]\n",
      "loss: 0.405966  [ 6416/60000]\n",
      "loss: 0.990879  [ 8016/60000]\n",
      "loss: 0.646799  [ 9616/60000]\n",
      "loss: 0.754296  [11216/60000]\n",
      "loss: 0.272261  [12816/60000]\n",
      "loss: 0.549629  [14416/60000]\n",
      "loss: 0.535209  [16016/60000]\n",
      "loss: 0.346451  [17616/60000]\n",
      "loss: 0.476454  [19216/60000]\n",
      "loss: 0.694926  [20816/60000]\n",
      "loss: 0.148199  [22416/60000]\n",
      "loss: 1.059889  [24016/60000]\n",
      "loss: 0.408627  [25616/60000]\n",
      "loss: 0.480981  [27216/60000]\n",
      "loss: 0.544513  [28816/60000]\n",
      "loss: 0.237548  [30416/60000]\n",
      "loss: 0.619708  [32016/60000]\n",
      "loss: 0.292487  [33616/60000]\n",
      "loss: 0.777915  [35216/60000]\n",
      "loss: 0.391694  [36816/60000]\n",
      "loss: 0.329894  [38416/60000]\n",
      "loss: 0.511341  [40016/60000]\n",
      "loss: 0.602548  [41616/60000]\n",
      "loss: 0.277951  [43216/60000]\n",
      "loss: 0.254559  [44816/60000]\n",
      "loss: 0.548878  [46416/60000]\n",
      "loss: 0.456543  [48016/60000]\n",
      "loss: 0.350074  [49616/60000]\n",
      "loss: 0.326118  [51216/60000]\n",
      "loss: 0.246200  [52816/60000]\n",
      "loss: 0.527568  [54416/60000]\n",
      "loss: 0.438197  [56016/60000]\n",
      "loss: 0.430499  [57616/60000]\n",
      "loss: 0.500683  [59216/60000]\n",
      "size=10000\n",
      "num_batches=625\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.518305 \n",
      "\n",
      "All Done !!!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(dataloader=train_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    test_loop(dataloader=test_dataloader, model=model, loss_fn=loss_fn)\n",
    "\n",
    "print(\"All Done !!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
